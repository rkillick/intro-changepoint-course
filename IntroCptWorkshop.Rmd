---
title: "Introduction to Changepoint Analysis"
author: "Rebecca Killick(r.killick@lancs.ac.uk)"
date: "CASI 2024"
output:
  beamer_presentation:
    includes:
      in_header: header.tex
    theme: lancaster
---

```{r,include=FALSE}
if(!require(changepoint)){
  install.packages('changepoint')
}
library(changepoint)
if(!require(changepoint.np)){
  install.packages('changepoint.np')
}
library(changepoint.np)
if(!require(EnvCpt)){
  install.packages('EnvCpt')
}
library(EnvCpt)
if(!require(changepoint.influence)){
  install.packages('changepoint.influence')
}
library(changepoint.influence)
knitr::opts_chunk$set(fig.align='center',fig.show='hold',fig.width=4,fig.height=4,size='footnotesize', cache=TRUE)
knitr::opts_knit$set(progress = FALSE,verbose = FALSE)
```

## Workshop Plan
* What are changepoints?
* Notation
* Likelihood based changepoints
    + Change in mean
    + Change in mean and/or variance
    + Change in trend and autocorrelation
* How many changes?
* Non-parametric changepoints
* Checking assumptions (if time allows)

There will be tasks throughout the sections.

## What are Changepoints?
Changepoints are also known as:

* breakpoints
* segmentation
* structural breaks
* regime switching
* detecting disorder

and can be found in a wide range of literature including

* quality control
* economics
* medicine
* environment
* linguistics
* \ldots


## What are changepoints?
For data $y_1, \ldots, y_n$, if a changepoint exists at $\tau$, then $y_1,\ldots,y_{\tau}$ differ from $y_{\tau+1},\ldots,y_n$ in some way.  

There are many different types of change.
  
```{r, echo=F, out.width='.3\\textwidth'}
par(mar=c(4,4,.3,.3)) 
set.seed(1)
# Change in mean example following EFK
x=1:500
y=c(rnorm(100,1,sd=0.5),rnorm(150,0,sd=0.5),rnorm(200,2,sd=0.5),rnorm(50,0.5,sd=0.5))
plot(x,y,type='l',xlab='',ylab='')
lines(x=1:100,y=rep(1,100),col='red',lwd=3)
lines(x=101:250,y=rep(0,150),col='red',lwd=3)
lines(x=251:450,y=rep(2,200),col='red',lwd=3)
lines(x=451:500,y=rep(0.5,50),col='red',lwd=3)
# Change in variance example following EFK
x=1:500
y=c(rnorm(100,0,sd=0.1),rnorm(150,0,sd=0.7),rnorm(200,0,sd=0.25),rnorm(50,0,sd=1))
plot(x,y,type='l',xlab='',ylab='')
# Change in regression
x=1:500
y=c(0.01*x[1:100],1.5-0.02*(x[101:250]-101),(10^-5)*(-150000+2.5*(x[251:450]^2-251^2)-(x[251:450]-250)),rep(1,50))
ynoise=y+rnorm(500,0,0.2)
plot(x,ynoise,type='l',xlab='',ylab='')
lines(x=1:100,y=0.01*x[1:100],lwd=3,col='red')
lines(x=101:250,y=1.5-0.02*(x[101:250]-101),lwd=3,col='red')
lines(x=251:450,y=(10^-5)*(-150000+2.5*(x[251:450]^2-251^2)-(x[251:450]-250)),lwd=3,col='red')
lines(x=451:500,y=rep(1,50),lwd=3,col='red')
```

## What is the goal?
* Has a change occurred?
* If yes, where is the change?
* What is the difference between the pre and post change data?
    + Maybe this is the type of change
    + Maybe it is the parameter values before and after the change
* What is the probability that a change has occured?
* How certain are we of the changepoint location?
* How many changes have occurred (+ all the above for each change)?
* Why has there been a change?

## Notation and Concepts
```{r, echo=F,out.height='.8\\textheight'}
par(mar = c(4, 4, 0.1, 0.1))
set.seed(1)
data=c(rnorm(100,0,1),rnorm(100,5,1),rnorm(100,0,1))
plot(data,xlab='',ylab='',xaxt='n',type='l')
axis(1,at=c(0,100,200,300),labels=c(0,expression(tau[1]),expression(tau[2]),300))
```

## Notation and Concepts
Thus a changepoint model for a change in mean has the following formulation:
$$
y_t = \left\{ \begin{array}{lcl} \mu_1 & \mbox{if} & 1\leq t \leq \tau_1 \\
          \mu_2 & \mbox{if} & \tau_1 < t \leq \tau_2 \\
          \vdots & & \vdots \\
          \mu_{m+1} & \mbox{if} & \tau_m < t \leq \tau_{m+1}=n \end{array} \right.
$$


## More complicated changes
```{r, echo=F,out.width='.45\\textwidth'}
set.seed(1)
par(mar=c(4,4,.3,.3)) 
# Change in ar example
x=1:500
y=c(arima.sim(model=list(ar=0.8),n=100),arima.sim(model=list(ar=c(0.5,0.2)),n=150),arima.sim(model=list(ar=c(-0.2)),n=200),arima.sim(model=list(ar=c(-0.5)),n=50))
plot(x,y,type='l',xlab='',ylab='')
# Change in seasonality and noise
x=1:500
y=c(sin(x[1:250]/21)+cos(x[1:250]/21),sin((1.1*x[251:500]/15))+cos((1.1*x[251:500]/15)))
ynoise=y+c(rnorm(100,0,sd=0.1),rnorm(150,0,sd=0.25),rnorm(200,0,sd=0.3),rnorm(50,0,sd=.4))
plot(x,ynoise,type='l',xlab='',ylab='')
```

## Online vs Offline
* Online
    + Processes data as it arrives or in batches
    + Goal is quickest detection of a change
    + Often used in processing control, intrusion detection
* Offline
    + Processes all the data in one go
    + Goal is accurate detection of a change
    + Often used in genome analysis, audiology


## Online vs Offline
```{r, echo=F,out.width='.45\\textwidth'}
set.seed(1)
par(mar=c(4,4,.3,.3)) 
x=1:110
y=c(rnorm(100),rnorm(10,2,1))
# online example
library(cpm)
cpm=detectChangePoint(y,cpmType="Student")
plot(x,y,type='n',xlab='',ylab='')
lines(x[1:cpm$detectionTime],y[1:cpm$detectionTime])
lines(x[(cpm$detectionTime+1):110],y[(cpm$detectionTime+1):110],lty=5,col='grey')
abline(v=cpm$changePoint)
abline(v=cpm$detectionTime,col='red')
#offline example
plot(x,y,type='l',xlab='',ylab='')
cpt=cpt.mean(y)
abline(v=cpts(cpt))
```


## Packages
Today we will use the

`library(changepoint)`

`library(changepoint.np)`

`library(EnvCpt)`

packages.

Other notable `R` packages are available for changepoint analysis including

* `strucchange` - for changes in regression
* `bcp` - if you want to be Bayesian
* `cpm` - for online changes (`changepoint.online` coming soon)
* `fastcpd` - for long series with few or clustered changepoints

## Single Changepoint
Assume we have time-series data where
$$
y_t|\theta_t \sim \mbox{N}(\theta_t,1),
$$
but where the means, $\theta_t$, are piecewise constant through time with a single change.



```{r, echo=FALSE, out.width='.5\\textwidth'}
# Change in mean example following EFK
x=1:500
y=c(rnorm(150,1,sd=0.5),rnorm(350,0,sd=0.5))
plot(x,y,type='l',xlab='',ylab='')
lines(x=1:150,y=rep(1,150),col='red',lwd=3)
lines(x=151:500,y=rep(0,350),col='red',lwd=3)
```


## Single Changepoint
\begin{center}
\includegraphics[width=0.9\textwidth]{SingleChange.pdf}
\end{center}
<!-- We want to infer the number and position of the points at which the mean changes. One approach: -->

<!-- **Likelihood Ratio Test** -->

<!-- To detect a single changepoint we can use the (log-)likelihood ratio test statistic: -->
<!-- $$ -->
<!-- LR=\max_\tau\{\ell(y_{1:\tau})+\ell(y_{\tau+1:n})-\ell(y_{1:n})\}. -->
<!-- $$ -->

<!-- We infer a changepoint if $LR>\lambda$ for some (suitably chosen) $\lambda$. If we infer a changepoint its position is estimated as  -->
<!-- $$ -->
<!-- \tau=\arg \max \{\ell(y_{1:\tau})+\ell(y_{\tau+1:n})-\ell(y_{1:n})\}. -->
<!-- $$ -->

## Finding a single change
```{r,fig.show='animate',echo=FALSE,out.height='0.6\\textheight'}
for(i in 1:499){
plot(x,y,type='l',xlab='',ylab='',main="499 options")
abline(v=i,col='red')
lines(x=1:i,y=rep(mean(y[1:i]),i),col='red',lwd=3)
lines(x=(i+1):500,y=rep(mean(y[(i+1):500]),500-i),col='red',lwd=3)
}
```

## Finding a single change
```{r,echo=FALSE,out.width='.5\\textwidth'}
    n=length(y)
    sumy=c(0,cumsum(y))
    sumy2=c(0,cumsum(y^2))
    taustar=1:n
    tmp=sumy2[taustar+1]-sumy[taustar+1]^2/taustar + (sumy2[n+1]-sumy2[taustar+1]) - ((sumy[n+1]-sumy[taustar+1])^2)/(n-taustar)
ts.plot(tmp,main=which.min(tmp))
abline(v=which.min(tmp),col='red')
```

## Finding a single change
How do we know if the changepoint found is "significant" or not?

* We also calculate the cost of the whole data with no change
* If the difference is **large enough** then we say there is a change

In practice **large enough** is hard to define as it is application dependent.  

The default in the `changepoint` package is MBIC - a Modified Bayesian Information Criterion.  

This will not be appropriate for all data sets!  More later $\ldots$

## `changepoint` R package
The `changepoint` R package contains 3 wrapper functions:

* `cpt.mean` - mean only changes
* `cpt.var` - variance only changes
* `cpt.meanvar` - mean and variance changes

The package also contains:

* functions/methods for the `cpt` S4 class
* 5 data sets
* other R functions that are made available for those who know what they are doing and might want to extend/modify the package.


## The `cpt` class
* S4 class
* Slots store all the information from the analysis
    + e.g. `data.set`, `cpts`, `param.est`, `pen.value`, `ncpts.max`
* Slots are accessed via their names e.g. `cpts(x)`
* Standard methods are available for the class e.g. `plot`, `summary`
* Additional generic functions are available e.g. `seg.len`, `ncpts`
* Each core function outputs a `cpt` object

##  `cpt.mean`
`cpt.mean(data, penalty="MBIC", pen.value=0, method="AMOC", Q=5, test.stat="Normal", class=TRUE, param.estimates=TRUE,minseglen=1)`

* `data` - vector or `ts` object
* `penalty` - cut-off point, MBIC, SIC, BIC, AIC, Hannan-Quinn, Asymptotic, Manual. 
* `pen.value` - Type I error for Asymptotic, number or character for manual.
* `method` - AMOC, PELT, SegNeigh, BinSeg.
* `Q` - max number of changes for SegNeigh or BinSeg.
* `test.stat` - Test statistic, Normal or CUSUM.
* `class` - return a `cpt` object or not.
* `param.estimates` - return parameter estimates or not.
* `minseglen` - minimum number of data points between changes.


## Single Change in Mean
*IMPORTANT*: The `cpt.mean` function assumes that the variance of a time series is 1. If this is not the case then you need to scale the data prior to analysis.

```{r, out.width='.3\\textwidth'}
set.seed(1)
m1=c(rnorm(100,0,1),rnorm(100,5,1))
m1.amoc=cpt.mean(m1)
cpts(m1.amoc)
```
## Single Change in Mean
```{r}
plot(m1.amoc)
```

<!-- ## Task: GP Weekly Appts. -->
<!-- Data from NHS Digital, weekly number of appointments (for all types) between Nov 2017 and Oct 2018. -->
<!-- ```{r,fig.height=3,fig.width=7,out.height='0.35\\textheight',out.width='\\textwidth'} -->
<!-- load('GPvisitsWeekNov1718.Rdata') -->
<!-- ts.plot(GPvisitsWeekNov1718) -->
<!-- ``` -->

<!-- Is there a change? -->

<!-- ## Task: GP Weekly Appts. -->
<!-- Use the  `cpt.mean` function to see if there is evidence for a change in mean in the weekly GP appointment data. -->
<!-- ```{r} -->
<!-- load('GPvisitsWeekNov1718.Rdata') -->
<!-- ``` -->
<!-- If you identify a change, where is it and what are the pre and post change means? -->

<!-- Don't forget to -->
<!-- ```{r} -->
<!-- library(changepoint) -->
<!-- ``` -->
<!-- before you start -->

<!-- ## Task: GP Weekly Appts. -->
<!-- ```{r} -->
<!-- GP.default=cpt.mean(GPvisitsWeekNov1718) -->
<!-- cpts(GP.default) -->
<!-- param.est(GP.default) -->
<!-- ``` -->

<!-- ## Task: GP Weekly Appts. -->
<!-- ```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'} -->
<!-- plot(GP.default) -->
<!-- ``` -->

<!-- ## Task: GP Weekly Appts. -->
<!-- ```{r} -->
<!-- GP.scale=cpt.mean(as.vector(scale(GPvisitsWeekNov1718))) -->
<!-- cpts(GP.scale) -->
<!-- ``` -->

## Task: Nile
Data from Cobb (1978): readings of the annual flow volume of the Nile River at Aswan from 1871 to 1970.
```{r,fig.height=3,fig.width=7,out.height='0.35\\textheight',out.width='\\textwidth'}
data(Nile)
ts.plot(Nile)
```

Hypothesized that there was a change around the turn of the century.


## Task: Nile
Use the  `cpt.mean` function to see if there is evidence for a change in mean in the Nile river data.
```{r}
data(Nile)
```
If you identify a change, where is it and what are the pre and post change means?

Don't forget to
```{r}
library(changepoint)
```
before you start

## Task: Nile
Annual flow volume of the Nile River at Aswan from 1871 to 1970 studied in Cobb(1978).
```{r}
nile.default=cpt.mean(Nile,method="AMOC")
cpts(nile.default)
cpts.ts(nile.default)
param.est(nile.default)
```

## Task: Nile
```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'}
plot(nile.default)
```

## 	Multiple Changepoints
```{r,echo=FALSE}
x=1:100
y=c(rnorm(20,1,sd=0.5),rnorm(30,0,sd=0.5),rnorm(40,2,sd=0.5),rnorm(10,0.5,sd=0.5))
plot(x,y,type='l',xlab='',ylab='')
```

## Multiple Changepoints
```{r,fig.show='animate',echo=FALSE,out.height='0.7\\textheight'}
# for(i in 1:97){
  for(j in (1):98){
    for(k in (j+1):99){
plot(x,y,type='l',xlab='',ylab='',main=paste(choose(length(y)-1,2),'options'))
# abline(v=c(i,j,k),col='red')
abline(v=c(j,k),col='red')
# lines(x=1:i,y=rep(mean(y[1:i]),i),col='red',lwd=3)
# lines(x=(i+1):j,y=rep(mean(y[(i+1):j]),j-i),col='red',lwd=3)
lines(x=1:j,y=rep(mean(y[1:j]),j),col='red',lwd=3)
lines(x=(j+1):k,y=rep(mean(y[(j+1):k]),k-j),col='red',lwd=3)
lines(x=(k+1):100,y=rep(mean(y[(k+1):100]),100-k),col='red',lwd=3)
    }
  }
# }
```
<!-- Define $m$ to be the number of changepoints, with positions $\bm{\tau}=(\tau_0,\tau_1,\ldots,\tau_{m+1})$ where $\tau_0=0$ and $\tau_{m+1}=n$. -->

<!-- Then one application of the Likelihood ratio test can be viewed as -->
<!-- $$ -->
<!-- \min_{m\in\{0,1\},\bm{\tau}} \left\{ -->
<!-- \sum_{i=1}^{m+1} \left[-\ell(y_{\tau_{i-1}:\tau_{i}})\right] + \lambda m \right\} -->
<!-- $$ -->
<!-- Repeated application is thus aiming to minimise -->
<!-- $$ -->
<!-- \min_{m,\bm{\tau}} \left\{ -->
<!-- \sum_{i=1}^{m+1} \left[-\ell(y_{\tau_{i-1}:\tau_{i}})\right] + \lambda m \right\} -->
<!-- $$ -->

<!-- ## Penalised Likelihood -->
<!-- The above can be viewed as a special case of penalised likelihood. Here the aim -->
<!-- is to maximise the *(log-)likelihood* over the number and position of the changepoints, but -->
<!-- *subject to* a penalty, that depends on the number of changepoints. The penalty is to avoid -->
<!-- over-fitting. -->

<!-- This is equivalent to minimising -->
<!-- $$ -->
<!-- \min_{m,\bm{\tau}} \left\{ -->
<!-- \sum_{i=1}^{m+1} \left[-\ell(y_{\tau_{i-1}:\tau_{i}})\right] + \lambda f(m) \right\} -->
<!-- $$ -->
<!-- for a suitable penalty function $f(m)$ and penalty constant $\lambda$. -->


<!-- ## Identifying changes? -->
<!-- All these methods can be cast in terms of minimising a function of $m$ and $\bm{\tau}$ of the form: -->
<!-- $$ -->
<!-- \sum_{i=1}^{m+1}{\left[\mathcal{C}(y_{(\tau_{i-1}+1):\tau_i})\right] + \beta f(m)}. -->
<!-- $$ -->

<!-- This function depends on the data just through a sum of a *cost* for each segment.  There is then a penalty term that depends on the number of segments. -->

<!-- ### Open Research Question -->
<!-- What penalty should I use? -->

<!-- Several have attempted to answer this question, but in reality have added their own criteria to the list.  At best, we have specific criteria shown to be optimal in very specific settings. -->


## The Challenge
* What are the values of $\tau_1,\ldots,\tau_m$?
* What is $m$?

* For $n$ data points there are $2^{n-1}$ possible solutions
* If $m$ is known there are still $\binom{n-1}{m}$ solutions
* If $n=1000$ and $m=10$,  $2.607755 \times 10^{23}$ solutions

* How do we search the solution space efficiently?


## Methods in `changepoint`
* At Most One Change (`AMOC`)

*Approximate* but computationally **fast**:

* Binary Segmentation (`BinSeg`) (Scott and Knott (1974)) which is $\mathcal{O}(n\log n)$ in CPU time.

*Slower* but **exact**:

* Segment Neighbourhood (`SegNeigh`) (Auger and Lawrence (1989)) is $\mathcal{O}(Qn^2)$.

**Fast** and **exact**:

* Pruned Exact Linear Time (`PELT`) (Killick et al. (2012)) At worst  $\mathcal{O}(n^2)$. For linear penalties<!-- $f(m)=m$ -->, scaling changes, $\mathcal{O}(n)$.


## cpt.var
`cpt.var(data, penalty, pen.value, know.mean=FALSE, mu=NA, method, Q, test.stat="Normal", class, param.estimates, minseglen=2)`

Majority of arguments are the same as for `cpt.mean`

* `know.mean` - if known we don't count it as an estimated parameter when calculating
penalties.
* `mu` - Mean if known.
* `test.stat` - Normal  or CSS (cumulative sums of squares)
* `minseglen` - Default is 2


## Changes in Variance
```{r,results='hold'}
set.seed(1)
v1=c(rnorm(100,0,1),rnorm(100,0,2),rnorm(100,0,10), 
     rnorm(100,0,9))
v1.man=cpt.var(v1,method='PELT',penalty='Manual',
     pen.value='2*log(n)')
cpts(v1.man)
param.est(v1.man)
```
## Changes in Variance
Ratios of true variances (4, 25, 0.81)
```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'}
plot(v1.man,cpt.width=3)
```

## `cpt.meanvar`
`cpt.meanvar(data, penalty, pen.value, method, Q, test.stat="Normal", class, param.estimates, shape=1,minseglen=2)`

Again the same underlying structure as `cpt.mean`.

* `test.stat` - choice of Normal, Gamma, Exponential, Poisson.
* `shape` - assumed shape parameter for Gamma.
* `minseglen` - minimum segment length of 2

## Mean & Variance
```{r}
set.seed(1)
mv1=c(rexp(50,rate=1),rexp(50,5),rexp(50,2),rexp(50,7))
mv1.pelt=cpt.meanvar(mv1,test.stat='Exponential',
      method='BinSeg',Q=10,penalty="SIC")
cpts(mv1.pelt)
param.est(mv1.pelt)
```

## Mean & Variance
```{r}
plot(mv1.pelt,cpt.width=3,cpt.col='blue')
```

<!-- ## FTSE100 -->
<!-- Yahoo! Finance data, daily returns from FTSE100 index. -->
<!-- 2nd April 1984 until the 13th September 2012 -->
<!-- ```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'} -->
<!-- data(ftse100) -->
<!-- plot(ftse100,type='l') -->
<!-- ``` -->


<!-- ## Task: FTSE100 -->
<!-- Use the  `cpt.var` function to see if there is evidence for changes in variance in the FTSE100 data. -->
<!-- ```{r} -->
<!-- data(ftse100) -->
<!-- ``` -->
<!-- If you identify changes, where are they and what are the variances in each segment? -->

## Task HC1
G+C content within part of Human Chromosome 1, data from NCBI.
3kb windows along the Human Chromosome from 10Mb to 33Mb.

Use the `cpt.meanvar` function to identify regions with different C+G content.

```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'}
data(HC1)
ts.plot(HC1)
```

<!-- ## A solution: FTSE100 -->
<!-- Yahoo! Finance data, daily returns from FTSE100 index. -->
<!-- ```{r} -->
<!-- data(ftse100) -->
<!-- ftse.man=cpt.var(ftse100[,2],method='PELT',minseglen=7) -->
<!-- ncpts(ftse.man) -->
<!-- ``` -->

<!-- ## A solution: FTSE100 -->
<!-- ```{r, fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'} -->
<!-- plot(ftse.man,ylab='Returns') -->
<!-- ``` -->

## A solution: HC1
```{r}
data(HC1)
hc1.pelt=cpt.meanvar(HC1,method='PELT',penalty='Manual',
		pen.value=14)
ncpts(hc1.pelt)
```
## A solution: HC1
```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'}
plot(hc1.pelt,ylab='G+C Content',cpt.width=3)
```


## Number of changes?
Does the number of changes appear reasonable?
```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'}
plot(hc1.pelt,ylab='G+C Content',cpt.width=3)
```

## CROPS
**C**hangepoints for a **r**ange **o**f **p**enaltie**s**

Use `penalty='CROPS'` with `method='PELT'` to get all segmentations for a range of penalty values.

```{r}
v1.crops=cpt.var(v1,method="PELT",penalty="CROPS",
                 pen.value=c(5,500))
```
## CROPS
```{r}
cpts.full(v1.crops)
```
## CROPS
```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'}
pen.value.full(v1.crops)
plot(v1.crops,ncpts=5)
```

## CROPS
```{r,fig.height=4,fig.width=4,out.height='0.7\\textheight'}
plot(v1.crops,diagnostic=TRUE)
```

<!-- ## Lavielle (2005) -->
<!-- For $1\leq K\leq K_{MAX}$: -->
<!-- $$ -->
<!-- J_K = \frac{\ell_{K_{MAX}}-\ell_K}{\ell_{K_{MAX}}-\ell_1} \left(K_{MAX}-1\right) + 1 -->
<!-- $$ -->
<!-- Then for $2\leq K\leq K_{MAX}-1$: -->
<!-- \begin{align} -->
<!-- D_K &= J_{K-1}-2J_K+J_{K+1} \\ -->
<!-- D_1 &= \infty -->
<!-- \end{align} -->
<!-- Then -->
<!-- $$ -->
<!-- \hat{K} = \max\{1\leq K\leq K_{MAX}-1 : D_K>C\} -->
<!-- $$ -->
<!-- $C$ is the threshold for a change. -->


## `cpt.np`
`cpt.np(data, penalty, pen.value, method, test.stat="empirical_distribution", class, minseglen=1, nquantiles=10)`

Again the same underlying structure as `cpt.mean`.

* `test.stat` - choice of empirical_distribution
* `minseglen` - minimum segment length of 1
* `nquantiles` - number of quantiles to use

## Example
```{r}
set.seed(12)
J <- function(x){(1+sign(x))/2}
n <- 1000
tau <- c(0.1,0.13,0.15,0.23,0.25,0.4,0.44,0.65,0.76,0.78,
      0.81)*n
h <- c(2.01, -2.51, 1.51, -2.01, 2.51, -2.11, 1.05, 2.16, 
      -1.56, 2.56, -2.11)
sigma <- 0.5
t <- seq(0,1,length.out = n)
data <- array()
for (i in 1:n){
   data[i] <- sum(h*J(n*t[i] - tau)) + (sigma * rnorm(1))
}
```

## Example
```{r}
out <- cpt.np(data, method="PELT",minseglen=2, 
        nquantiles =4*log(length(data)))
cpts(out)
```
## Example
```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'}
plot(out)
```

<!-- ## Task: CROPS -->
<!-- Look at the FTSE100 data again and use the CROPS technique to determine an appropriate number of changes. -->

<!-- ## Task: Air Temps -->
<!-- ```{r,echo=FALSE} -->
<!-- airtemp=rev(c(5.37,-0.67,-1.30,NaN,-4.80,-4.60,-3.43,-4.20,-5.17,-6.50,-3.83,NaN,-5.43,-3.73,-7.73,-7.00,-4.63,-7.27,-0.40,-5.53,-4.03,-6.00,-3.07,-2.23,-2.90,-6.93,-7.07,-6.10,-6.60,-6.80,-6.47,-4.50,-7.23,-5.27,-7.47,-8.00,-2.53,-2.83,-5.30,NaN,-4.33,-6.33,-7.27,-7.47,-8.77,-6.47,-3.80,-7.00,-6.40,-8.60,-4.77,-3.37,-5.20,-5.10,-7.50)) -->
<!-- ``` -->
<!-- Yearly air temperatures from Fedorova, AMJ glacier. 3 missing values -->
<!-- Use the `cpt.meanvar` function and CROPS to identify the number of changes. -->

<!-- ```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'} -->
<!-- plot(seq(from=1959,to=2013,by=1),airtemp,type='l') -->
<!-- ``` -->

<!-- Hint: To remove the NAs use `airtemp[!is.na(airtemp)]`. -->

<!-- ## Task: Sea Ice -->
<!-- ```{r,echo=FALSE} -->
<!-- seaice=c(5.61,10.09,7.54,9.68,5.72,4.83,5.25,6.09,6.29,6.18,5.2,5.56,6.07,4.09,4.72,2.98,3.27,5.27,0.82,9.43,5.35,4.49,4.37,4.13,4.65,4.97,5.17,4.54,5.49,2.65,2.23,2.09,1.65,1.75,2.12,1.28) -->
<!-- ``` -->
<!-- Yearly Sea Ice measurements for July-Sept for Barents from 1979 until 2014. -->

<!-- Use the `cpt.meanvar` function and CROPS to identify the number of changes. -->
<!-- ```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'} -->
<!-- plot(seq(from=1979,to=2014,by=1),seaice,type='l') -->
<!-- ``` -->


## Task: `cpt.np`
Look at the `HeartRate` data from the `changepoint.np` package.  Use one of the non-parametric functions to see if there is evidence for changes in heart rate.
```{r}
data(HeartRate)
```

<!-- ## A solution: FTSE100 -->
<!-- ```{r} -->
<!-- ftse.crops=cpt.var(ftse100[,2],method='PELT', -->
<!--     penalty='CROPS',pen.value=c(5,1000)) -->
<!-- ``` -->
<!-- ## A solution: FTSE100 -->
<!-- ```{r,out.height='0.55\\textheight'} -->
<!-- plot(ftse.crops,diagnostic=TRUE) -->
<!-- abline(v=11,col='red') -->
<!-- abline(v=34,col='red') -->
<!-- ``` -->
<!-- ## A solution: FTSE100 -->
<!-- ```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'} -->
<!-- plot(ftse.crops,ncpts=11) -->
<!-- ``` -->
<!-- ## A solution: FTSE100 -->
<!-- ```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'} -->
<!-- plot(ftse.crops,ncpts=34) -->
<!-- ``` -->

<!-- ## A solution: Air Temps -->
<!-- ```{r} -->
<!-- airtemp.crops=cpt.meanvar(airtemp[!is.na(airtemp)],method='PELT', -->
<!--           penalty='CROPS',pen.value=c(1,100)) -->
<!-- ``` -->
<!-- ## A solution: Air Temps -->
<!-- ```{r} -->
<!-- plot(airtemp.crops,diagnostic=TRUE) -->
<!-- abline(v=1,col='red') -->
<!-- ``` -->
<!-- ## A solution: Air Temps -->
<!-- ```{r} -->
<!-- plot(airtemp.crops,ncpts=1) -->
<!-- ``` -->

<!-- ## A solution: Sea Ice -->
<!-- ```{r} -->
<!-- seaice.crops=cpt.meanvar(seaice,method="PELT", -->
<!--       penalty="CROPS",pen.value=c(1,100)) -->
<!-- ``` -->

<!-- ## A solution: Sea Ice -->
<!-- ```{r,out.height='0.7\\textheight'} -->
<!-- plot(seaice.crops,diagnostic=TRUE) -->
<!-- abline(v=2,col='red') -->
<!-- abline(v=4,col='red') -->
<!-- ``` -->

<!-- ## A solution: Sea Ice -->
<!-- ```{r} -->
<!-- plot(seaice.crops,ncpts=2) -->
<!-- ``` -->

<!-- ## A solution: Sea Ice -->
<!-- ```{r} -->
<!-- plot(seaice.crops,ncpts=4) -->
<!-- ``` -->

## A solution: HeartRate
```{r}
HR.pelt=cpt.np(HeartRate,method='PELT',
               nquantiles=4*log(length(HeartRate)))
ncpts(HR.pelt)
```
## A solution: HeartRate
```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'}
plot(HR.pelt)
```
## A solution: HeartRate
```{r}
HR.crops=cpt.np(HeartRate, penalty = "CROPS", 
        pen.value = c(5,200), method="PELT",minseglen=2,
        nquantiles =4*log(length(HeartRate)))
```
## A solution: HeartRate
```{r, out.height='0.5\\textheight'}
plot(HR.crops, diagnostic = TRUE)
abline(v=11,col='red')
abline(v=15,col='red')
```
## A solution: HeartRate
```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'}
plot(HR.crops, ncpts = 11)
```
## A solution: HeartRate
```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'}
plot(HR.crops, ncpts = 15)
```

## Trend and Autocorrelation

```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'}
load('temperature_anomalies_updated.RData')
plot(Tanom_annual_df$year,Tanom_annual_df$HadCRUT,type='l')
```

## T+AC Model

A simple linear trend and AR(1) model would be:

$$
y_t = \left\{ \begin{array}{lcl} \alpha_1 + \beta_1 t + \epsilon_t & \mbox{if} & 1\leq t \leq \tau_1 \\
          \alpha_2 + \beta_2 t + \epsilon_t & \mbox{if} & \tau_1 < t \leq \tau_2 \\
          \vdots & & \vdots \\
          \alpha_{m+1} + \beta_{m+1} + \epsilon_t & \mbox{if} & \tau_m < t \leq \tau_{m+1}=n \end{array} \right.
$$
where
$$
\epsilon_t = \left\{ \begin{array}{lcl} \phi_{1,1} (y_t-\alpha_1 - \beta_1 t) + Z_t & \mbox{if} & 1\leq t \leq \tau_1 \\
          \phi_{2,1} (y_t-\alpha_2 - \beta_2 t) + Z_t & \mbox{if} & \tau_1 < t \leq \tau_2 \\
          \vdots & & \vdots \\
          \phi_{m+1,1} (y_t-\alpha_{m+1} - \beta_{m+1} t) + Z_t & \mbox{if} & \tau_m < t \leq \tau_{m+1}=n \end{array} \right.
$$

and $Z_t \sim N(0,\sigma^2)$.

## `EnvCpt`

AIM: select the most parsimonious but accurate model for the data.
  
\begin{center}
  \includegraphics[height=5.5cm]{Models.pdf}
\end{center}

Simple to extend with other types of models.

## `EnvCpt`
`envcpt(data,models=c("mean","meancpt","meanar1","meanar2",`
`"meanar1cpt","meanar2cpt","trend","trendcpt","trendar1",`
`"trendar2","trendar1cpt","trendar2cpt"), minseglen=5,...,`
`verbose=TRUE)`

* data is a vector or `ts` object
* models is a character vector of models to fit, default is all
* minseglen is the minimum number of observations between changes
* verbose prints progress through model fitting
* $\ldots$ are any other parameters to the `cpt.*` functions

Returns S3 object, has AIC and plot methods.

## Task: Annual Temperatures
```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'}
load('temperature_anomalies_updated.RData')
plot(Tanom_annual_df$year,Tanom_annual_df$HadCRUT,type='l')
```

## Task: Annual Temperatures
Use the  `envcpt` function to see if there is evidence for a change in the temperature data.
```{r}
load('temperature_anomalies_updated.RData')
```
The data is available in `Tanom_annual_df` with several groups (columns) to choose from.

What is the best model?  How many changepoints?

Don't forget to
```{r}
library(EnvCpt)
```
before you start.

## A solution: NASA

```{r}
library(EnvCpt)
nasa.envcpt=envcpt(Tanom_annual_df$NASA[
    !is.na(Tanom_annual_df$NASA)], verbose=F)
which.min(AIC(nasa.envcpt))
Tanom_annual_df$year[!is.na(Tanom_annual_df$NASA)
                ][cpts(nasa.envcpt$trendar1cpt)]
```

## A solution: NASA
```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'}
plot(nasa.envcpt$trendar1cpt)
abline(v=cpts(nasa.envcpt$trendar1cpt))
```

## A solution: NASA
```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'}
plot(nasa.envcpt)
```

## Checking Assumptions
The main assumptions for a Normal likelihood ratio test for a change in mean are:

* Independent data points;
* Normal distributed points pre and post change;
* Constant variance across the data.

How can we check these?

## Checking Assumptions
In reality we can't check assumptions prior to analysis.
```{r,out.height='0.5\\textheight'}
ts.plot(m1)
```
## Checking Assumptions
```{r,out.height='0.6\\textheight'}
hist(m1)
```

## Checking Assumptions
```{r}
shapiro.test(m1)
ks.test(m1,pnorm,mean=mean(m1),sd=sd(m1))
```

## Checking Assumptions
```{r,out.height='0.6\\textheight'}
acf(m1)
library(WeightedPortTest)
Weighted.Box.test(m1, lag=min(50,length(m1)/4))$p.value
```

<!-- ## How to check -->
<!-- * Check each segment independently -->
<!-- ```{r} -->
<!-- cpt.seg=cbind(c(0,cpts(m1.amoc)),seg.len(m1.amoc)) -->
<!-- data=data.set(m1.amoc) -->
<!-- shapiro.func=function(x){ -->
<!--   out=shapiro.test(data[(x[1]+1):(x[1]+x[2])]) -->
<!--   return(c(out$statistic,p=out$p.value))} -->
<!-- apply(cpt.seg,1,shapiro.func) -->
<!-- ``` -->
<!-- ## Segment Check -->
<!-- ```{r} -->
<!-- ks.func=function(x){ -->
<!--   tmp=data[(x[1]+1):(x[1]+x[2])] -->
<!--   out=ks.test(tmp,pnorm,mean=mean(tmp),sd=sd(tmp)) -->
<!--   return(c(out$statistic,p=out$p.value))} -->
<!-- apply(cpt.seg,1,ks.func) -->
<!-- ``` -->
<!-- ## Segment Check -->
<!-- ```{r,out.width='0.45\\textwidth',out.height='0.5\\textheight'} -->
<!-- qqnorm.func=function(x){ -->
<!--   qqnorm(data[(x[1]+1):(x[1]+x[2])]) -->
<!--   qqline(data[(x[1]+1):(x[1]+x[2])])} -->
<!-- out=apply(cpt.seg,1,qqnorm.func) -->
<!-- ``` -->
<!-- ## Segment Check -->
<!-- ```{r,out.width='0.45\\textwidth',out.height='0.5\\textheight'} -->
<!-- acf.func=function(x){ -->
<!--   acf(data[(x[1]+1):(x[1]+x[2])])} -->
<!-- out=apply(cpt.seg,1,acf.func) -->
<!-- ``` -->

## How to check
* Check the residuals
```{r}
means=param.est(m1.amoc)$mean
m1.resid=m1-rep(means,seg.len(m1.amoc))
shapiro.test(m1.resid)
```

## Residual Check
```{r}
ks.test(m1.resid,pnorm,mean=mean(m1.resid),sd=sd(m1.resid))
```

## Residual Check
```{r,out.height='0.6\\textheight'}
qqnorm(m1.resid)
qqline(m1.resid)
```

## Residual Check
```{r,out.height='0.6\\textheight'}
acf(m1.resid)
Weighted.Box.test(m1.resid,
            lag=min(50,length(m1.resid)/4))$p.value
```

## Task
Check the assumptions you have made on the simulated, Nile, <!-- FTSE100  Air Temps GP Appointments, -->HC1, HeartRate, and Temperature data using the residual check. 

What effect might any invalid assumptions have on the inference?


## Influence

* Which data points are *influential* for obtaining the segmentation? 
  * Changepoints versus Outliers
  * How to measure influence?
* How *stable* is the obtained segmentation?

\mbox{}

Note: Currently the package considers mean changes but we are working on expansions.  The premise is applicable to all model and changepoint forms.  It is just the calulation of what you ``expect" to happen that can be tricky.

## Influence: Example
```{r, out.height='0.6\\textheight'}
set.seed(30) 
x=c(rnorm(50),rnorm(50,mean=5),rnorm(1,mean=15),
  rnorm(49,mean=5),rnorm(50,mean=4))
xcpt=cpt.mean(x,method='PELT')
plot(xcpt,cpt.width=3,ylab='')
```
## How to measure?
Sources of Inspiration:

* Regression Analysis: Measures of Influence (e.g., Cook's distance)
* Robust Statistics: Influence Functions

Two routes:

* Modifying an observation
* Leaving out an observation

## Modify
\includegraphics[scale=0.45]{changemeanoutlier} 

## Stability Dashboard: Out
```{r, out.height='0.7\\textheight'}
x.inf.out=influence(xcpt,method='outlier')
out.Stability=StabilityOverview(x,cpts(xcpt),x.inf.out,
  legend.args=list(display=TRUE,x="topright",y=NULL,cex=1,
  horiz=FALSE,xpd=FALSE,bty='n'))
```

## Location Stability: Out
```{r, out.height='0.7\\textheight'}
out.location=LocationStability(cpts(xcpt),x.inf.out,
  type='Difference')
```

## Parameter Stability: Out
```{r, out.height='0.7\\textheight'}
ParameterStability(x.inf.out,original.mean=rep(
  param.est(xcpt)$mean,times=diff(c(0,xcpt@cpts))))
```

## Influence Map: Out

```{r, out.height='0.7\\textheight'}
out.map=InfluenceMap(cpts(xcpt),x.inf.out)
```

## Stability Dashboard: Del
```{r, out.height='0.6\\textheight'}
x.inf.del=influence(xcpt,method='delete')
del.Stability=StabilityOverview(x,cpts(xcpt),x.inf.del,
  legend.args=list(display=TRUE,x="topright",y=NULL,cex=1,
  horiz=FALSE,xpd=FALSE,bty='n'))
```

## Location Stability: Del
```{r, out.height='0.7\\textheight'}
del.location=LocationStability(cpts(xcpt),x.inf.del,
  type='Difference')
```

## Parameter Stability: Del
```{r, out.height='0.7\\textheight'}
ParameterStability(x.inf.del,original.mean=rep(
  param.est(xcpt)$mean,times=diff(c(0,xcpt@cpts))))
```

## Influence Map: Del

```{r, out.height='0.7\\textheight'}
del.map=InfluenceMap(cpts(xcpt),x.inf.del)
```

## Task: Nile Influence

```{r}
nile.cpts=cpt.mean(Nile,penalty="Manual",
                   pen.value=100000,method="PELT")
```

Look at the influence on the simulated, and Nile. 

What effect might any invalid assumptions have on the influences you are seeing?

## Task: Nile Influence

```{r,out.height='0.6\\textheight'}
nile.inf=influence(nile.cpts,method="outlier")
stab=StabilityOverview(data.set(nile.cpts), cpts(nile.cpts),
    nile.inf,legend.args=list(display=T,x="topright", 
    horiz=T,y=NULL,cex=1,bty="n",xpd=F))
```

## Task: Nile Influence

```{r,out.height='0.6\\textheight'}
ParameterStability(nile.inf, original.mean=rep(
  param.est(nile.cpts)$mean,times=diff(c(0,nile.cpts@cpts))))
```

## Task: Nile Influence

```{r,out.height='0.6\\textheight'}
locstab=LocationStability(cpts(nile.cpts), nile.inf, 
            type="Difference")
```


## Nile Influence

```{r,out.height='0.6\\textheight'}
inf=InfluenceMap(cpts(nile.cpts),nile.inf)
```

## Consolidating Task
Download the ratings for the following TV shows from the [IMDB](http://www.imdb.com/) and analyze the series using some of the techniques you have learnt from today.  For each series, do you identify any changes?  Are the assumptions you are making valid? What effect might any invalid assumptions have on the inference? Do the changepoints remain stable under influence?

* [Doctor Who](http://www.imdb.com/title/tt0436992/epdate)
* [Grey's Anaytomy](http://www.imdb.com/title/tt0413573/epdate)
* [Mistresses](http://www.imdb.com/title/tt2295809/epdate)
* [The Simpsons](http://www.imdb.com/title/tt0096697/epdate)
* [Top Gear](http://www.imdb.com/title/tt1628033/epdate)

(Understandably IMBD does not allow screen scraping nor downloads of information for redistribution so you will have to copy and paste the table into Excel, or equivalent, yourself in order to get the ratings data into R.)

## Bonus
Just from looking at the data, can you predict which shows have been cancelled?

## References
[JSS:](https://www.jstatsoft.org/article/view/v058i03) Killick, Eckley (2014)  

[PELT:](http://www.tandfonline.com/doi/abs/10.1080/01621459.2012.737745) Killick, Fearnhead, Eckley (2012)  

[CROPS:](http://dx.doi.org/10.1080/10618600.2015.1116445) Kaynes, Eckley, Fearnhead (2015)  

[cpt.np:](http://link.springer.com/article/10.1007/s11222-016-9687-5) Haynes, Fearnhead, Eckley (2016)

[EnvCpt:](https://doi.org/10.1175/JCLI-D-17-0863.1) Beaulieu, Killick (2018) 

[Influence:](https://www.tandfonline.com/doi/full/10.1080/10618600.2021.2000873) Wilms, Killick, Matteson (2022)

<!-- [Lavielle](http://dx.doi.org/10.1016/j.sigpro.2005.01.012) (2005) -->

